POSSIBLE GENERAL APPROACH:

Figure out how to do things "correctly" first, then (later) figure out clever
speedups (e.g., compute filter values properly first; later, look into using
lookup table as in PBRT) which can be tested against results of slow-but-proper
approach.



POSSIBLE THINGS TO DO:

[] Camera Class
	instantiated from scene file
		Needs to have default instantiation in case scene file doesn't specify
		a camera!
	stores info about tanTheta, FOV, aspect ratio
	returns rays using method like GenerateCameraRay

Instead of generating image during Render(), accumulate an array of "samples"
(x, y, color info) and use a later function to assemble the output image, possibly
via image-reconstruction filter (see PBRT book).
	[] Write Python code to apply image-reconstruction filter to set of samples
	[] Figure out possible Sample or SampleArray class
	
	Possibly useful reference code:
	https://github.com/wjakob/nori/blob/master/src/rfilter.cpp
	
	Note that PBRT updates the individual pixel values when a sample is computed,
	so the samples are never stored. The updating uses a lookup-table of filter
	values, effectively rounding the actual radius-to-center-of-pixel
	(It also stores the cumulative weights per pixel, and then divides each pixel
	by its associated weight when the rendering is done.)


** Revised Scene implementation
	[x]vector of pointers to Object base class
		subclasses: Sphere, Plane
	[x]vector of pointers to Light base class
		subclasses: Point, Distant
	vector of pointers to Material base class
	camera object


[..]Implement lights.
		[X] Implement DistantLight
		Implement spotlight? (see PBRT)


[X]Implement plane with intersection.

Implement box object with intersection


Implement camera re-orientation (see scratchapixel.com v2 notes: camera is at
(0,0,0) in its native frame; translation to (x_cam,y_cam,z_cam) in world frame + 
rotation.
	Simplest approach may be to follow scratchapixel: compute everything as
	currently done, then transform pixel position from camera space to world
	space, subtract from (transformed) camera position to get direction ray in
	world space.


Save alpha channel.


Save depth channel.


? Replace Mersenne Twister with faster RNG?
	e.g., https://github.com/wjakob/pcg32/blob/master/pcg32.h


WILD FUTURE IDEAS:

Adaptive sub-sampling
	Compare intensities of samples for a given pixel; if dispersion is too large,
	generate extra samples for that pixel

	
Read in OBJ files? (e.g., with tinyobj library)

Read in (simple) RIB files??


(Uni-directional) path tracing??



ASSORTED NOTES:

Direction vectors:
	Vector pointing from point A to point B is:
		B - A = (x_b - x_a, y_b - y_a, z_b - z_a)
		
		Ex: direction vector from origin to point P:
		dir(O->P) = P - O = (x_p - 0, y_p - 0, z_p - 0) = (x_p, y_p, z_p) [i.e., same components as P]


*** Image orientation:
	Internally, (x,y) = (0,0) becomes the *upper left* pixel in the output image
	(e.g., PNG displayed in Preview)
		==> small x = left side of image
		    smally = *upper* part of image



*** Camera rays:

	Image plane = (forward-projected) focal plane, located at z = 1 from camera
	origin (z = -1 in camera coordinate space)
	Camera origin = location of pinhole/lens
	
	The camera ray has its origin at the camera position (more precisely, at the
	camera *pinhole/lens*, and a direction defined by subtracting origin from current 
	pixel coordinate (output from Sampler), after the pixel coordinate has been 
	transformed from image ("raster") space to world space.
		raydir = pt_pixel_world - origin_cam_world
	
	Converting 2D sample coordinate in raster space to 3D position in camera space
	1. 2D Raster-space coordinates: x_pix, y_pix (generated by Sampler object)
								spans (0--npix_x - 1, 0--npix_y - 1)
	2. 2D Normalized device coordinates (NDC; (spans 0--1,0--1)): x_ndc, y_ndc 
			x_ndc = (x_pix + 0.5)/width
			y_ndc = (y_pix + 0.5)/height
	3. 2D Screen space (spans -1--1, -1--1): x_scr, y_scrn
			x_scrn = 2*x_ndc - 1
			y_scrn = -(2*y_ndc - 1)   [negative so we get y_scrn = -1 at *bottom* of image]
	4. 3D Camera space coordinates: x_cam, y_cam, z_cam
		(note that by construction, the camera is oriented to point out along the
		-z axis, with the screen 1 unit away, so all screen points are at z = -1)
			x_cam = x_scrn * tanTheta * aspectRatio
			y_cam = y_scrn * tanTheta
			z_cam = -1

	Thus,
		raydir = (x_cam, y_cam, z_cam) - (0,0,0) -- in camera coordinates



*** Lights and light falloff/attenuation (acc. to PBRT)

The standard for light transmitted by rays is "radiance" (= astronomical *intensity*),
which remains constant along rays (through open space) regardless of distance.
This applies to all light reflected from (or refracted through) surfaces.

For distant lights, the light value (internal data member "L") *is*
radiance, and is unaffected by distance.
Same for diffuse lights (internal data member "Lemit").

For *point lights*, the light value (internal data member "L") is
equivalent to luminosity, and *is* affected by distance in the expected
1/d^2 fashion.

For spotlights, the light value (internal data member "Intensity") is also
equivalent to luminosity, with same 1/d^2 falloff as for point light. (I think
the idea is that a spotlight is essentially a point light with emission
restricted to a cone.)

PBRT's GonioPhotometricLight behaves same as point/spot light in this sense;
again, I think the idea is that it's a point light with a specific angular
emission function.



*** Translating photometric terms:

PBRT		other			astronomy
=====================================
xxx							---
xxx							luminosity
irradiance					flux
xxx							---
radiance					intensity
