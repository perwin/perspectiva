POSSIBLE GENERAL APPROACH:

Figure out how to do things "correctly" first, then (later) figure out clever
speedups (e.g., compute filter values properly first; later, look into using
lookup table as in PBRT) which can be tested against results of slow-but-proper
approach.



POSSIBLE THINGS TO DO:

Instead of generating image during Render(), accumulate an array of "samples"
(x, y, color info) and use a later function to assemble the output image, possibly
via image-reconstruction filter (see PBRT book).
	[] Write Python code to apply image-reconstruction filter to set of samples
	[] Figure out possible Sample or SampleArray class
	
	Possibly useful reference code:
	https://github.com/wjakob/nori/blob/master/src/rfilter.cpp
	
	Note that PBRT updates the individual pixel values when a sample is computed,
	so the samples are never stored. The updating uses a lookup-table of filter
	values, effectively rounding the actual radius-to-center-of-pixel
	(It also stores the cumulative weights per pixel, and then divides each pixel
	by its associated weight when the rendering is done.)


** Revised Scene implementation
	[x]vector of pointers to Object base class
		subclasses: Sphere, Plane
	[x]vector of pointers to Light base class
		subclasses: Point, Distant
	vector of pointers to Material base class
	camera object


[..]Implement lights.
		[ ] Implement DistantLight


Implement plane and/or box object with intersection.


Implement camera re-orientation (see scratchapixel.com v2 notes: camera is at
(0,0,0) in its native frame; translation to (x_cam,y_cam,z_cam) in world frame + 
rotation.
	Simplest approach may be to follow scratchapixel: compute everything as
	currently done, then transform pixel position from camera space to world
	space, subtract from (transformed) camera position to get direction ray in
	world space.


Save alpha channel.


Save depth channel.


? Replace Mersenne Twister with faster RNG?
	e.g., https://github.com/wjakob/pcg32/blob/master/pcg32.h


WILD FUTURE IDEAS:

Adaptive sub-sampling
	Compare intensities of samples for a given pixel; if dispersion is too large,
	generate extra samples for that pixel

	
Read in OBJ files? (e.g., with tinyobj library)

Read in (simple) RIB files??


(Uni-directional) path tracing??



ASSORTED NOTES:

Direction vectors:
	Vector pointing from point A to point B is:
		B - A = (x_b - x_a, y_b - y_a, z_b - z_a)
		
		Ex: direction vector from origin to point P:
		dir(O->P) = P - O = (x_p - 0, y_p - 0, z_p - 0) = (x_p, y_p, z_p) [i.e., same components as P]


*** Image orientation:
	Internally, (x,y) = (0,0) becomes the *upper left* pixel in the output image
	(e.g., PNG displayed in Preview)
		==> small x = left side of image
		    smally = *upper* part of image



*** Camera rays:

	Image plane = (forward-projected) focal plane, located at z = 1 from camera
	origin (z = -1 in camera coordinate space)
	Camera origin = location of pinhole/lens
	
	The camera ray has its origin at the camera position (more precisely, at the
	camera *pinhole/lens*, and a direction defined by subtracting origin from current 
	pixel coordinate (output from Sampler), after the pixel coordinate has been 
	transformed from image ("raster") space to world space.
		raydir = pt_pixel_world - origin_cam_world
	
	Converting 2D sample coordinate in raster space to 3D position in camera space
	1. 2D Raster-space coordinates: x_pix, y_pix (generated by Sampler object)
								spans (0--npix_x - 1, 0--npix_y - 1)
	2. 2D Normalized device coordinates (NDC; (spans 0--1,0--1)): x_ndc, y_ndc 
			x_ndc = (x_pix + 0.5)/width
			y_ndc = (y_pix + 0.5)/height
	3. 2D Screen space (spans -1--1, -1--1): x_scr, y_scrn
			x_scrn = 2*x_ndc - 1
			y_scrn = -(2*y_ndc - 1)   [negative so we get y_scrn = -1 at *bottom* of image]
	4. 3D Camera space coordinates: x_cam, y_cam, z_cam
		(note that by construction, the camera is oriented to point out along the
		-z axis, with the screen 1 unit away, so all screen points are at z = -1)
			x_cam = x_scrn * tanTheta * aspectRatio
			y_cam = y_scrn * tanTheta
			z_cam = -1

	Thus,
		raydir = (x_cam, y_cam, z_cam) - (0,0,0) -- in camera coordinates



*** Translating photometric terms:

PBRT		other			astronomy
=====================================
xxx							---
xxx							luminosity
xxx							flux
xxx							---
xxx							intensity
