POSSIBLE GENERAL APPROACH:

Figure out how to do things "correctly" first, then (later) figure out clever
speedups (e.g., compute filter values properly first; later, look into using
lookup table as in PBRT) which can be tested against results of slow-but-proper
approach.


*** CURRENT PROBLEM:

Implementing Transforms inside Sphere::intersect is currently not
working, probably because calculated t_ca value is opposite to what the
(naive) algorithm expects. I think this is because the algorithm
assumes "Sphere in front of camera" = positive t_ca, but if we use
object coords, where Sphere is at (0,0,0) and camera is at -z values,
then t_ca ends up negative.


*** BUGS:

[] Visible lights aren't (don't seem to be visible, and don't seem to cast light)


[] When using OpenMP, there are artifacts not present in non-OpenMP version.
Specifically, our newscene.yml scene ends up with scattered dark gray to black
pixels in certain rows (y = constant); this seems to be related to
shadow-ray tracing (i.e., these seem to be pixels which end up with
partial or complete "blocked" results, even though they are nowhere near
the actual shadows).



*** CURRENT ISSUES AND POTENTIAL THINGS TO DO

[] OpenMP ("scons --openmp raytracer2") works, but certain rows will end up with
dark (shadowed?) pixels -- see above. Presumably there's something going on
involving different threads writing to a shared variable that shouldn't be
shared, and it maybe seems to happen when tracing shadow rays.


[] Change: Successful intersection tests return an object with intersection info
(instead of just t_0 and t_1):
	-- pointer to Shape object
	-- t_0, t_1
	-- intersection point in world space
	-- normal at intersection (n_hit)
	Rewrite intersect() to return std::optional ? (C++-17 equiv. of Haskell Maybe)
		https://www.bfilipek.com/2018/05/using-optional.html
		[x]clang++ -std=c++17
	[ ] sketch out IntersectionInfo class
	[ ] ID points in code where IntersectionInfo class would be used/returned/etc.


[] unittest_geometry has weird issues with the tests of the Ray class


[X] Add option to "trace" processing for just one pixel
	-- i.e., user specifies a pixel, and program does rendering for just that
	pixel, with lots of logging


[] Finish implementing rectangle with intersection



*** POSSIBLE THINGS TO DO:

[ ] Environment map with equirectangular maps
	[ ] Unit tests
		[ ] Locate cube-map unit tests (if they exist)
		[ ] Write unit tests for mapping Ray to lat,long
		[ ] Write unit tests for mapping lat,long to image x,y
	[ ] Modify scenefile_parser to process equirectangular image
	
		
		
*[..] Updated materials
	render.cpp -- RayTrace()
		IF refraction and/or reflection:
			intersectedShape->reflection -- scalar
			intersectedShape->transparency -- scalar
			intersectedShape->GetSurfaceColor() -- this colors the reflection *and* refraction
		ELSE:
			intersectedShape->ComputeShapeColor(raydir, n_hit, lightDirection);
	
	Shape::ComputeShapeColor
		Material *shapeMaterial
		
    reflection = shapeMaterial->GetReflectivity();
    transparency = shapeMaterial->GetTransparency();

	*[ ] Implement new material class
		[x] Create new branch
			$ git checkout -b new-material
		
		[x] Rewrite materials.h to specify new class
			
		[ ] Re/write Material unit tests
			
		[x] Update scenefile_parser with new Material interface
			
		[x] Update Shapes classes with new Material interface
			
		[ ] Create new test scene file
			
	

[..] Revise code in shapes.h & scenefile_parser.cpp so that default initializations create
a default Material, rather than taking assorted material-related inputs
	-- Preferred usage will be to assign a named material in the scene file
	[x] Do this for Spheres
	scene.h: 
		AddSphereToScene_new
		AddSphereToScene
	[x] Do this for Plane
		AddPlaneToScene
	[x] Do this for Box
		AddBoxToScene
	[x] Edit scene files used by regress_test.sh
	
	[x] Update Scene::AssembleDefaultScene

	[x] Update Scene::AssembleTestScene

	
[ ] Implement rotations (via Transform)
	[ ] Implement for Box class
		[ ] Figure out how to implement location normalization
			-- current Box class stores & uses world (x,y,z) for corners
			-- to properly handle rotations, we should work with nominal location
			centered at (0,0,0) in object coords.
			-- Intersection test should transform input Ray into object space.


[ ] Python code to generate scene files
	-- e.g., generate N spheres, randomly scattered.

[..] Introduce use of Ray class
	[X]RayTrace()
	[X]Camera::GenerateCameraRay
	Shape::intersect ??
	TraceShadowRay() ??


[ ] Bokey?
	-- implement non-circular shapes for thin lens
	[ ] Create working examples of (standard circular) bokeh
		-- e.g., scene file with multiple distant, out-of-focus, visible lights
	

[ ] Better handling of multiple refractions
	-- our current code handles case of isolated refracting object inside
	scene, but can't correctly handle case of one refracting object inside
	another refracting object (e.g., glass sphere inside water, or vice-versa)
	-- in principle, we might be able to keep track of what we are inside/outside
	of by tracking (in the Ray instance) what kind of normals we encounter:
		1. Start in scene ("outside all"): "inside = 0", ior = [ior_scene]
		2. Intersect with object; new refraction ray will pass inside object
			3. Refraction ray has "inside = 1", ior = [ior_scene, ior_obj1]
			
			[] Reach opposite side of object (passing from inside to outside)
				pop ior --> ior = [ior_scene]
				inside -= 1 --> "inside = 0"
			OR: Reach outside of second object (inside first)
				ior -> [ior_scene, ior_obj1, ior_obj2]
				inside += 1 --> "inside = 2"
	
	SOME CASES TO CONSIDER:
		Single sphere: outside to inside to outside
			ior_scene -> ior_sphere -> ior_scene
		Nested objects: outside to inside 1st to inside 2nd to inside 1st to outside
			ior_scene -> ior_1 -> ior_2 -> ior_1 -> ior_scene
		Parallel slabs: outside to inside 1st to inside 2nd (but outside 1st) to outside
			ior_scene -> ior_1 -> ior_2 -> ior_scene
		
	-- What we really want to be able to do is, when we reach an intersection,
	is peek ahead and see what the IOR of the neighboring material is
		-- For scene-to-inside-object transitions, this is easy
		-- For one-object-to-another-object transitions, this is trickier,
		since the intersection is (likely to be) with the further boundary
		of the first object; how do we determine the IOR of the next object?
		-- For inside-one-object-to-scene, we have the same problem
		
	-- Possible test: store ID of shape we've entered in Ray
		If intersection:
			Check to see if ID Of intersecting object is same
				YES: ==> OK, we've reached further boundary of object and are about to
				move outside it -- either into another object, or into the surrounding scope
					==> continue ray and look for next intersection with t > t_intersection
					Check to see if t_new - t_intersection < delta
						YES: consider the next object to be adjacent
							==> get its IOR; store its ID in refractive sub-Ray
						NO: we really are moving into the surrounding scope
						
				NO: ==> we've reached a new object (possibly inside first object)
						 ==> get its IOR; store its ID in refractive sub-Ray
				
	(How do we handle 2D objects like Planes? Maybe best to simply say we shouldn't
	assign refractive materials to 2D objects -- they must be opaque.)


	

Instead of generating image during Render(), accumulate an array of "samples"
(x, y, color info) and use a later function to assemble the output image, possibly
via image-reconstruction filter (see PBRT book).
	[] Write Python code to apply image-reconstruction filter to set of samples
	[] Figure out possible Sample or SampleArray class
	
	Possibly useful reference code:
	https://github.com/wjakob/nori/blob/master/src/rfilter.cpp
	
	Note that PBRT updates the individual pixel values when a sample is computed,
	so the samples are never stored. The updating uses a precomputed lookup-table of filter
	values, effectively rounding the actual radius-to-center-of-pixel values.
	(It also stores the cumulative weights per pixel, and then divides each pixel
	by its associated weight when the rendering is done.)
	This same approach is implemented in the Nori raytracer referenced immediately
	above.
	https://github.com/wjakob/nori/blob/master/src/block.cpp
	
	[ ] Implement filtering using filter calculation (e.g., Gaussian)
	
	[ ] Replace formal filter calculation with lookup-table of precomputed values,
	a al PBRT
	


** [x] Add classes for Points and Vectors
	[x] Rename current geometry.h/cpp as shapes
		[x] Rename file (and stuff within, if neeeded)
		[x] Rename 'import "geometry.h"' --> 'import "shapes.h"'
	[x] unittest_geometry.t.h & run_unittests_geometry.sh
		[x] Rename & endit
		[x] Run tests
	[x] compile program & run regression tests
	
	[x] Create geometry.h(cpp?) file & unit tests for Points, Vectors, Rays
		[x] Create unittest_geometry.t.h
		[x] Create run_unittest_geometry.sh
		
		[x] Define Point class
		[x] Write unit tests for Point class, and code to pass them
			[x] Write basic unit tests
			[x] Write more unit tests

		[x] Define Vector class
		[x] Write unit tests for Vector class, and code to pass them
			[x] Write basic unit tests
			[x] Write more unit tests
			
		[x] Write dot-product function for vectors

		[x] Define Ray class
		[x] Write unit tests for Ray class, and code to pass them
			[x] Write basic unit tests
			[x] Write more unit tests

	[x] Rework code to use Point/Vector
		Vec3f --> Point or Vector
		vec3f.normalize() --> Normalize(vec3f)
		substitute for updated dot-product function
		
		[x] ID files needing updating
			[x] materials.h
			[x] shapes.h
			[x] unittest_shapes.t.h

			[x] cameras.h
				position, direction, GenerateCameraRay
			[x] lights.h
			[x] unittest_lights.t.h
			[x] scene.h
			[x] shapes.cpp
			[x] render.cpp
			[x] scenefile_parser.cpp


	1. Replace Vec3f with Point or Vector, as appropriate
	2. LATER: replace camera origin, raydir, depth with Ray


Preliminary transformation matrices
	-- probably best to base this on PBRT approach
	
	1. Initial implementation using null/stub transforms (which return positions
	and vectors unmodified)
	2. Replace this with matrix-transformation, but still using null transform
	
	Intersection tests of camera rays with objects
		-- simplest to do this in *object local* space
		Convert from world coords (ray origin, direction) to object coords
	Test using spheres
		-- only positional offsets should matter
		-- easy comparison with existing renderings
	[ ] Add to Shape class: test for presence of Transform pointer; if not
	present, default to no-op
	
	[ ] NOTE: Current code generally assumes that objects are in *front* of
	camera (e.g, Sphere intersection tests for t_ca >= 0 to see if intersection
	is in front of camera). But when we use WorldToObject inside the Intersect
	methods, directions are *reversed* -- i.e., instead of camera ray origin
	at (0,0,0) and sphere's center at (0, -1, -30), the ray origin is at (0, -1, -30)
	and the sphere's center is at (0,0,0). This means that the returned t
	value for a hit is wrong, and so RayTrace()'s calculation of p_hit will
	be wrong...
	
	[ ] Also need to modify Sphere::GetNormalAtPoint (and for other subclasses)
	so it internally converts the input Point from world to object coords.


[..]Implement lights.
		[X] Implement DistantLight
		Implement spotlight? (see PBRT)


[..] Images as textures
	[ ] Code to read and store an image
		[ ] Write interface
		[ ] Write simple unit test
			[ ] Write code for unit test
			[ ] Generate simple (4x4 RGB?) test image
			[ ] Write code to make unit test pass
	
	[ ] Image as background
	
	[ ] Image as texture for plane


[ ] Modernize Shape constructors to remove "surfColor" argument (replace
with Material?)


[ ] Move color computation into Object (or its Material instance)
	-- needed for diffuse color:
		object/material intrinsic surfaceColor
		n_hit
		lightDirection
	-- needed for specular color:
		incoming ray direction (raydir)
		estimate of reflection ray direction (refldir)
			Vec3f refldir = raydir - n_hit*2*raydir.dotProduct(n_hit);

	So: input to material's ComputeColor function:
		raydir, n_hit, lightDirection
		-- and then we can compute everything in the Material object
	And then in RayTrace(), we multiply the result by (lightIntensity * visibility)
	to get the final (non-reflection,non-refraction) Color value
	
	Color MatteMaterial::ComputeObjectColor( Vec3f rayDirection, Vec3f n_hit, Vec3f lightDirection )
	{
      return surfaceColor * std::max(float(0), n_hit.dotProduct(-lightDirection));
	}		

	Color PhongMaterial::ComputeObjectColor( Vec3f rayDirection, Vec3f n_hit, Vec3f lightDirection )
	{
      Color diffuseColor = surfaceColor * std::max(float(0), n_hit.dotProduct(-lightDirection));
      Vec3f refldir = rayDirection - n_hit*2*rayDirection.dotProduct(n_hit);  // reflection direction
      refldir.normalize();
      Color specularColor = std::pow(std::max(0.f, refldir.dotProduct(-rayDirection)), phongExponent);
      
      return Kd*diffuseColor + Ks*specularColor;
	}		


[X]Implement plane with intersection.


Implement disk with intersection.


Implement box object with intersection


[ ] Implement visible lights (for area lights)
		SphereList and RectLight (and DiskLight, if we make that) should be visible
		Need to be part of object list as objects with emissivity
	

Implement camera re-orientation (see scratchapixel.com v2 notes: camera is at
(0,0,0) in its native frame; translation to (x_cam,y_cam,z_cam) in world frame + 
rotation.
	Simplest approach may be to follow scratchapixel: compute everything as
	currently done, then transform pixel position from camera space to world
	space, subtract from (transformed) camera position to get direction ray in
	world space.


Save alpha channel.


Save depth channel.


[-] Replace our Mersenne Twister with C++11 library implementation? (random::mt19937)
	-- NO: this is too arcane and complicated, for no
	-- besides, we might want to later try using pcg32, which is both faster,
	arguably better RNG, *and* easier to use.
	
? Replace Mersenne Twister with faster RNG?
	e.g., https://github.com/wjakob/pcg32/blob/master/pcg32.h


WILD FUTURE IDEAS:

Attenuation for transparent materials via Beer's Law (see notes in render.cpp)
	Need to check to see how attenuation coefficients map to observed color of
	tinted materials (water, tinted glass, etc.)
	Look in pbrt code?
	
Adaptive sub-sampling
	Compare intensities of samples for a given pixel; if dispersion is too large,
	generate extra samples for that pixel

	
Read in OBJ files? (e.g., with tinyobj library)

Read in (simple) RIB files??


(Uni-directional) path tracing??



ASSORTED NOTES:

World/scene orientation:

	For default camera orientation (e.g., for scene files, etc.):
		x-axis: left-right, increasing to right
		y-axis: up-down, increasing upwards [NOTE: this is *opposite* to pixel coords on image]
		z-axis: in-out, increasing *outward* (toward viewer); distance *into* scene,
			away from viewer, is *negative*
		
		This follows proper right-hand rule: +x vector x +y vector = +z vector
		

Direction vectors:
	+++ Vector pointing from A to B is B - A: +++
		B - A = (x_b - x_a, y_b - y_a, z_b - z_a)
		
		Ex: direction vector from origin to point P:
		dir(O->P) = P - O = (x_p - 0, y_p - 0, z_p - 0) = (x_p, y_p, z_p) [i.e., same components as P]


*** Image orientation:
	Internally, (x,y) = (0,0) becomes the *upper left* pixel in the output image
	(e.g., PNG displayed in Preview)
		==> small x = left side of image
		    small y = *upper* part of image



*** Camera rays:

	Image plane = (forward-projected) focal plane, located at z = 1 from camera
	origin (z = -1 in camera coordinate space)
	Camera origin = location of pinhole/lens
	
	The camera ray has its origin at the camera position (more precisely, at the
	camera *pinhole/lens*, and a direction defined by subtracting origin from current 
	pixel coordinate (output from Sampler), after the pixel coordinate has been 
	transformed from image ("raster") space to world space.
		raydir = pt_pixel_world - origin_cam_world
	
	Converting 2D sample coordinate in raster space to 3D position in camera space
	1. 2D Raster-space coordinates: x_pix, y_pix (generated by Sampler object)
								spans (0--npix_x - 1, 0--npix_y - 1)
	2. 2D Normalized device coordinates (NDC; (spans 0--1,0--1)): x_ndc, y_ndc 
			x_ndc = (x_pix + 0.5)/width
			y_ndc = (y_pix + 0.5)/height
	3. 2D Screen space (spans -1--1, -1--1): x_scr, y_scrn
			x_scrn = 2*x_ndc - 1
			y_scrn = -(2*y_ndc - 1)   [negative so we get y_scrn = -1 at *bottom* of image]
	4. 3D Camera space coordinates: x_cam, y_cam, z_cam
		(note that by construction, the camera is oriented to point out along the
		-z axis, with the screen 1 unit away, so all screen points are at z = -1)
			x_cam = x_scrn * tanTheta * aspectRatio
			y_cam = y_scrn * tanTheta
			z_cam = -1

	Thus,
		raydir = (x_cam, y_cam, z_cam) - (0,0,0) -- in camera coordinates


*** Area lights:

Idea is to do Monte Carlo integration to solve the problem: how much of the
area light's area can I actually see from a given point (taking into account
possible occlusion by other objects).

Ideally, we want to sample uniformly (and probably randomly) from the
projected surface of the area light. 

This is easy to do for a sphere, since its projected surface is always a circle.
	Sampling: ray to center of sphere + random offset, such that offset is
	always < R
	
	Easiest approach for raytracer case is probably to randomly generate *points on the
	surface of the sphere*, so that we can compute ray directions as p_surfsphere - p_0
	(where p_0 is the current position we're investigating)
	Strictly speaking we should generate points on the hemisphere facing the point,
	but because of the sphere's symmetry, it doesn't matter if we do the much
	easier thing and generate random points on the entire surface
	
	Random point on sphere = center + vector
		vector = random direction with length R
		
		OK, the correct way to do this is not obvious; 
		see here: http://mathworld.wolfram.com/SpherePointPicking.html
		1. Generate random variables theta between 0 and 2 pi and u [= cos phi] between -1 and 1
		2. x = sqrt(1 - u^2) * cos(theta)
		   y = sqrt(1 - u^2) * sin(theta)
		   z = u
		
		To get vector with length R, just do
			dir = R * Vec3f(x, y, z)
		   


*** Lights and light falloff/attenuation (acc. to PBRT)

The standard for light transmitted by rays is "radiance" (= astronomical *intensity*),
which remains constant along rays (through open space) regardless of distance.
This applies to all light reflected from (or refracted through) surfaces.

For distant lights, the light value (internal data member "L") *is*
radiance, and is unaffected by distance.
Same for diffuse lights (internal data member "Lemit").

For *point lights*, the light value (internal data member "L") is
equivalent to luminosity, and *is* affected by distance in the expected
1/d^2 fashion.

For spotlights, the light value (internal data member "Intensity") is also
equivalent to luminosity, with same 1/d^2 falloff as for point light. (I think
the idea is that a spotlight is essentially a point light with emission
restricted to a cone.)

PBRT's GonioPhotometricLight behaves same as point/spot light in this sense;
again, I think the idea is that it's a point light with a specific angular
emission function.

It appears that for *area* lights, the light value is *radiance* (think of
how surface brightness relates to flux: more distance galaxy has same
surface brightness, but occupies smaller angle, thus smaller received flux).



*** Translating photometric terms:

PBRT		other			astronomy
=====================================
xxx							---
xxx							luminosity
irradiance					flux
xxx							---
radiance					intensity



*** DEBUGGING WITH LLDB

https://lldb.llvm.org/use/tutorial.html

Compile with clang++, being sure to use "-g" option (which we are).

Invoke "lldb" with name of executable; use " -- " first to enable command-line
arguments for the executable:

$ lldb -- ./raytracer2 [options and args]

(lldb) run

If something halts the program (e.g., segfault)

(lldb) frame variable
	-- will list values of most current variables (but not, e.g., class members)

(lldb) frame variable <name>
	-- will list value of that variable (including class members)
E.g.
(lldb) frame variable skyboxImages[5][1000000]
	-- to get *that* particular value




*** BUGS FIXED:

[X] Currently, when a light (Point, probably other) has too low a y value for its
position, there's a weird cutoff in the illumination
this seems to happen when there is a sphere *and* a plane; but without the
plane, the sphere is illuminated correctly!
	[] Check to see if occulted pixels are *blocked* (and if so, by what object),
	or if something else is going on 
	
	HERE'S what seems to be happening:
		Points sufficiently high on sphere that their lightDirection vectors
		point down (dy < 0) have TraceShadowRay intersections with the *plane*
		(object index j = 1) where, for some reason, 
			t_0,t_1 = 26.701248,-437964930662619152384.000000
		The t_1 value is WRONG; since it is negative and therefore
		less than the distance to the light, this triggers blocked=true
		
		So the problem may be with the intersect method of the Plane class...
		
		OK, it appears that Plane::intersect never sets a value for t_1 if there's
		an intersection!
		This is a problem, because *if* there's an intersection, the code in
		TraceShadowRay compares the values of *both* t_0 *and* t_1 to the distance
		to the light.
		
		SOLUTION: Always set t1 in intersect() if an intersection is found; if no
		second intersection is possible, then set t1 = t0

EXAMPLE:
./raytracer2 --width=150 --height=100 tests/scene_sphere-point-light-left.yml -o test_point-left_big.exr


upper-left part of sphere (bad):  x = 5, y = 4:  lightDirection = (0.919428,0.012680,0.393054):  blocked = 1
lower-left part of sphere (good):  x = 5, y = 6:  lightDirection = (0.858244,-0.433943,-0.274064):  blocked = 0



*** DONE (AT LEAST FOR NOW):


[x] Add materials to scene
	-- ./run_unittest_scenefile_parser.sh
	[x] Add parsing of materials to scencefile_parser.cpp
		[x] Add parsing of "material" entries
			[x] Add parsing in AddMaterialToScene
			[x] Write AddSimpleMaterialToScene method to Scene class
				[x] Add stub method that does nothing
				[x] Write full method
			[x] CURRENT PROBLEM: This all seems to work, but then the unit
			test seg-faults
	
	[x] Add optional "material: " slot to .yml definition for shapes
	
	[x] Add assignation of materials to shapes
		-- After scene construction is finished, go through list of shapes
		and check to see if they had a material name assigned; if so
		assign the Material * corresponding to that name as the material
		for the shape


[X] Get refraction working properly
	[x] Get IOR from intersected shape
	[x] Compute refraction ray using correct IORs
		[x] Abstract refraction-direction computation to separate function
			[x] Write simple unit tests (ior1 = ior2)
			[x] Write more unit tests
	[X] Implement new refraction function in RayTrace


[X] Depth-of-field
	[X] Parsing of extra Camera info
		[X] Write unit test (in unittest_scenefile_parser.t.h) for processing
			of extra camera info (focalDist, aperture)
		[X] Write code to pass test
	
	[X] Implementation of depth-of-field
		http://www.pbr-book.org/3ed-2018/Camera_Models/Projective_Camera_Models.html
		[X] Update Camera::GenerateCameraRay and code using it so it generates a proper Ray
		[X] Look up best way to do this
		[X] Simple hard-wired test
			[X] Make scene file with multiple spheres at different distances


[X] Camera Class
	[x]instantiated from scene file
		Needs to have default instantiation in case scene file doesn't specify
		a camera!
	[x]stores info about tanTheta, FOV, aspect ratio
	[x]stores Sampler object (or pointer to same) ?
		in scene file: optional parameter (defaults to uniform)
	[x]returns rays using method like GenerateCameraRay
	
	[X] Add ability to instantiate camera from scene file
		[X] Unit tests
		[X] Add ability to read camera characteristics from scene file
		[x] Rewrite code in Render() to get pointer to Camera instance from scene
		file


[X]New Ray class
	Currently defined in geometry.h, but not yet used
	origin (point vector)
	direction vector
	most recent IOR passed through (for helping to determine refraction)
		Use: when generating a refraction ray, compute refraction using IOR
		of object/scene ray is going into and IOR of what the ray passed
		through just before hitting the refraction point.
		Non-ray/path-tracing Rays have currentIOR = 0 or somesuch;
		Camera rays have currentIOR = scene IOR.


** Revised Scene implementation
	[x]vector of pointers to Object base class
		subclasses: Sphere, Plane
	[x]vector of pointers to Light base class
		subclasses: Point, Distant
	vector of pointers to Material base class
	[x]camera object


[X] Implement box/rectangular solid object with intersection


