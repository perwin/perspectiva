POSSIBLE GENERAL APPROACH:

Figure out how to do things "correctly" first, then (later) figure out clever
speedups (e.g., compute filter values properly first; later, look into using
lookup table as in PBRT) which can be tested against results of slow-but-proper
approach.


*** BUGS:

Currently, when a light (Point, probably other) has too low a y value for its
position, there's a weird cutoff in the illumination
this seems to happen when there is a sphere *and* a plane; but without the
plane, the sphere is illuminated correctly!
	[] Check to see if occulted pixels are *blocked* (and if so, by what object),
	or if something else is going on 


POSSIBLE THINGS TO DO:

[..] Camera Class
	instantiated from scene file
		Needs to have default instantiation in case scene file doesn't specify
		a camera!
	stores info about tanTheta, FOV, aspect ratio
	returns rays using method like GenerateCameraRay
	
	[] Add ability to instantiate camera from scene file
		[] Unit tests
		[] Add ability to read camera characteristics from scene file
		[] in Render(), check to see if scene has camera; if not, instantiate
		default camera

Instead of generating image during Render(), accumulate an array of "samples"
(x, y, color info) and use a later function to assemble the output image, possibly
via image-reconstruction filter (see PBRT book).
	[] Write Python code to apply image-reconstruction filter to set of samples
	[] Figure out possible Sample or SampleArray class
	
	Possibly useful reference code:
	https://github.com/wjakob/nori/blob/master/src/rfilter.cpp
	
	Note that PBRT updates the individual pixel values when a sample is computed,
	so the samples are never stored. The updating uses a lookup-table of filter
	values, effectively rounding the actual radius-to-center-of-pixel
	(It also stores the cumulative weights per pixel, and then divides each pixel
	by its associated weight when the rendering is done.)


** Revised Scene implementation
	[x]vector of pointers to Object base class
		subclasses: Sphere, Plane
	[x]vector of pointers to Light base class
		subclasses: Point, Distant
	vector of pointers to Material base class
	camera object


New Ray class
	origin (point vector)
	direction vector
	most recent IOR passed through (for helping to determine refraction)
		Use: when generating a refraction ray, compute refraction using IOR
		of object/scene ray is going into and IOR of what the ray passed
		through just before hitting the refraction point.
		Non-ray/path-tracing Rays have currentIOR = 0 or somesuch;
		Camera rays have currentIOR = scene IOR.

[..]Implement lights.
		[X] Implement DistantLight
		Implement spotlight? (see PBRT)


[X]Implement plane with intersection.


Implement disk with intersection.


Implement box object with intersection


[ ] Implement visible lights (for area lights)
		SphereList and RectLight (and DiskLight, if we make that) should be visible
		Need to be part of object list as objects with emissivity
	

Implement camera re-orientation (see scratchapixel.com v2 notes: camera is at
(0,0,0) in its native frame; translation to (x_cam,y_cam,z_cam) in world frame + 
rotation.
	Simplest approach may be to follow scratchapixel: compute everything as
	currently done, then transform pixel position from camera space to world
	space, subtract from (transformed) camera position to get direction ray in
	world space.


Save alpha channel.


Save depth channel.


? Replace our Mersenne Twister with C++11 library implementation? (random::mt19937)

? Replace Mersenne Twister with faster RNG?
	e.g., https://github.com/wjakob/pcg32/blob/master/pcg32.h


WILD FUTURE IDEAS:

Adaptive sub-sampling
	Compare intensities of samples for a given pixel; if dispersion is too large,
	generate extra samples for that pixel

	
Read in OBJ files? (e.g., with tinyobj library)

Read in (simple) RIB files??


(Uni-directional) path tracing??



ASSORTED NOTES:

World/scene orientation:

	For default camera orientation (e.g., for scene files, etc.):
		x-axis: left-right, increasing to right
		y-axis: up-down, increasing upwards [NOTE: this is *opposite* to pixel coords on image]
		z-axis: in-out, increasing *outward* (toward viewer); distance *into* scene,
			away from viewer, is *negative*
		
		This follows proper right-hand rule: +x vector x +y vector = +z vector
		

Direction vectors:
	+++ Vector pointing from A to B is B - A: +++
		B - A = (x_b - x_a, y_b - y_a, z_b - z_a)
		
		Ex: direction vector from origin to point P:
		dir(O->P) = P - O = (x_p - 0, y_p - 0, z_p - 0) = (x_p, y_p, z_p) [i.e., same components as P]


*** Image orientation:
	Internally, (x,y) = (0,0) becomes the *upper left* pixel in the output image
	(e.g., PNG displayed in Preview)
		==> small x = left side of image
		    small y = *upper* part of image



*** Camera rays:

	Image plane = (forward-projected) focal plane, located at z = 1 from camera
	origin (z = -1 in camera coordinate space)
	Camera origin = location of pinhole/lens
	
	The camera ray has its origin at the camera position (more precisely, at the
	camera *pinhole/lens*, and a direction defined by subtracting origin from current 
	pixel coordinate (output from Sampler), after the pixel coordinate has been 
	transformed from image ("raster") space to world space.
		raydir = pt_pixel_world - origin_cam_world
	
	Converting 2D sample coordinate in raster space to 3D position in camera space
	1. 2D Raster-space coordinates: x_pix, y_pix (generated by Sampler object)
								spans (0--npix_x - 1, 0--npix_y - 1)
	2. 2D Normalized device coordinates (NDC; (spans 0--1,0--1)): x_ndc, y_ndc 
			x_ndc = (x_pix + 0.5)/width
			y_ndc = (y_pix + 0.5)/height
	3. 2D Screen space (spans -1--1, -1--1): x_scr, y_scrn
			x_scrn = 2*x_ndc - 1
			y_scrn = -(2*y_ndc - 1)   [negative so we get y_scrn = -1 at *bottom* of image]
	4. 3D Camera space coordinates: x_cam, y_cam, z_cam
		(note that by construction, the camera is oriented to point out along the
		-z axis, with the screen 1 unit away, so all screen points are at z = -1)
			x_cam = x_scrn * tanTheta * aspectRatio
			y_cam = y_scrn * tanTheta
			z_cam = -1

	Thus,
		raydir = (x_cam, y_cam, z_cam) - (0,0,0) -- in camera coordinates


*** Area lights:

Idea is to do Monte Carlo integration to solve the problem: how much of the
area light's area can I actually see from a given point (taking into account
possible occlusion by other objects).

Ideally, we want to sample uniformly (and probably randomly) from the
projected surface of the area light. 

This is easy to do for a sphere, since its projected surface is always a circle.
	Sampling: ray to center of sphere + random offset, such that offset is
	always < R
	
	Easiest approach for raytracer case is probably to randomly generate *points on the
	surface of the sphere*, so that we can compute ray directions as p_surfsphere - p_0
	(where p_0 is the current position we're investigating)
	Strictly speaking we should generate points on the hemisphere facing the point,
	but because of the sphere's symmetry, it doesn't matter if we do the much
	easier thing and generate random points on the entire surface
	
	Random point on sphere = center + vector
		vector = random direction with length R
		
		OK, the correct way to do this is not obvious; 
		see here: http://mathworld.wolfram.com/SpherePointPicking.html
		1. Generate random variables theta between 0 and 2 pi and u [= cos phi] between -1 and 1
		2. x = sqrt(1 - u^2) * cos(theta)
		   y = sqrt(1 - u^2) * sin(theta)
		   z = u
		
		To get vector with length R, just do
			dir = R * Vec3f(x, y, z)
		   


*** Lights and light falloff/attenuation (acc. to PBRT)

The standard for light transmitted by rays is "radiance" (= astronomical *intensity*),
which remains constant along rays (through open space) regardless of distance.
This applies to all light reflected from (or refracted through) surfaces.

For distant lights, the light value (internal data member "L") *is*
radiance, and is unaffected by distance.
Same for diffuse lights (internal data member "Lemit").

For *point lights*, the light value (internal data member "L") is
equivalent to luminosity, and *is* affected by distance in the expected
1/d^2 fashion.

For spotlights, the light value (internal data member "Intensity") is also
equivalent to luminosity, with same 1/d^2 falloff as for point light. (I think
the idea is that a spotlight is essentially a point light with emission
restricted to a cone.)

PBRT's GonioPhotometricLight behaves same as point/spot light in this sense;
again, I think the idea is that it's a point light with a specific angular
emission function.

It appears that for *area* lights, the light value is *radiance* (think of
how surface brightness relates to flux: more distance galaxy has same
surface brightness, but occupies smaller angle, thus smaller received flux).



*** Translating photometric terms:

PBRT		other			astronomy
=====================================
xxx							---
xxx							luminosity
irradiance					flux
xxx							---
radiance					intensity



upper-left part of sphere (bad):  x = 5, y = 4:  lightDirection = (0.919428,0.012680,0.393054):  blocked = 1
lower-left part of sphere (good):  x = 5, y = 6:  lightDirection = (0.858244,-0.433943,-0.274064):  blocked = 0

