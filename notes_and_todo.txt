POSSIBLE GENERAL APPROACH:

Figure out how to do things "correctly" first, then (later) figure out clever
speedups (e.g., compute filter values properly first; later, look into using
lookup table as in PBRT) which can be tested against results of slow-but-proper
approach.


*** CURRENT PROBLEM:

Implementing Transforms inside Sphere::intersect is currently not
working, probably because calculated t_ca value is opposite to what the
(naive) algorithm expects. I think this is because the algorithm
assumes "Sphere in front of camera" = positive t_ca, but if we use
object coords, where Sphere is at (0,0,0) and camera is at -z values,
then t_ca ends up negative.

[ ] Write some Python code to compute intersections and transforms...

[ ] Implement basic logging (spdlog)

	[x] Copy spdlog header files into directory; add to repo
	[x] Basic logging setup in main
	[x] Add headers to render.cpp


*** BUGS:



*** POSSIBLE THINGS TO DO:

[ ] Add materials to scene
	-- ./run_unittest_scenefile_parser.sh
	[x] Add parsing of materials to scencefile_parser.cpp
		[x] Add parsing of "material" entries
			[x] Add parsing in AddMaterialToScene
			[x] Write AddSimpleMaterialToScene method to Scene class
				[x] Add stub method that does nothing
				[xx] Write full method
			[x] CURRENT PROBLEM: This all seems to work, but then the unit
			test seg-faults
	
	[ ] Add optional "material: " slot to .yml definition for shapes
	
	[ ] Add assignation of materials to shapes
		-- After scene construction is finished, go through list of shapes
		and check to see if they had a material name assigned; if so
		assign the Material * corresponding to that name as the material
		for the shape


[ ] Python code to generate scene files
	-- e.g., generate N spheres, randomly scattered.

[ ] Introduce use of Ray class
	RayTrace()
	Camera::GenerateCameraRay
	Shape::intersect


[ ] Depth-of-field
	[ ] Parsing of extra Camera info
		[ ] Write unit test (in unittest_scenefile_parser.t.h) for processing
			of extra camera info (focalDist, aperture)
		[ ] Write code to pass test
	
	[ ] Implementation of depth-of-field
		http://www.pbr-book.org/3ed-2018/Camera_Models/Projective_Camera_Models.html
		[ ] Update Camera::GenerateCameraRay and code using it so it generates a proper Ray
		[ ] Look up best way to do this
		[ ] Simple hard-wired test
			[ ] Make scene file with multiple spheres at different distances
			[ ] 


[ ] Updated materials
	render.cpp -- RayTrace()
		IF refraction and/or reflection:
			intersectedShape->reflection -- scalar
			intersectedShape->transparency -- scalar
			intersectedShape->GetSurfaceColor() -- this colors the reflection *and* refraction
		ELSE:
			intersectedShape->ComputeShapeColor(raydir, n_hit, lightDirection);
	
	Shape::ComputeShapeColor
		Material *shapeMaterial
		
    reflection = shapeMaterial->GetReflectivity();
    transparency = shapeMaterial->GetTransparency();


	[ ] Try implementing a checkerboard for plane
	
	[ ] Try implementing metal shader
	
	

[..] Camera Class
	[x]instantiated from scene file
		Needs to have default instantiation in case scene file doesn't specify
		a camera!
	[x]stores info about tanTheta, FOV, aspect ratio
	[x]stores Sampler object (or pointer to same) ?
		in scene file: optional parameter (defaults to uniform)
	[x]returns rays using method like GenerateCameraRay
	
	[X] Add ability to instantiate camera from scene file
		[X] Unit tests
		[X] Add ability to read camera characteristics from scene file
		[x] Rewrite code in Render() to get pointer to Camera instance from scene
		file

Instead of generating image during Render(), accumulate an array of "samples"
(x, y, color info) and use a later function to assemble the output image, possibly
via image-reconstruction filter (see PBRT book).
	[] Write Python code to apply image-reconstruction filter to set of samples
	[] Figure out possible Sample or SampleArray class
	
	Possibly useful reference code:
	https://github.com/wjakob/nori/blob/master/src/rfilter.cpp
	
	Note that PBRT updates the individual pixel values when a sample is computed,
	so the samples are never stored. The updating uses a precomputed lookup-table of filter
	values, effectively rounding the actual radius-to-center-of-pixel values.
	(It also stores the cumulative weights per pixel, and then divides each pixel
	by its associated weight when the rendering is done.)
	This same approach is implemented in the Nori raytracer referenced immediately
	above.


** Revised Scene implementation
	[x]vector of pointers to Object base class
		subclasses: Sphere, Plane
	[x]vector of pointers to Light base class
		subclasses: Point, Distant
	vector of pointers to Material base class
	[x]camera object


** [x] Add classes for Points and Vectors
	[x] Rename current geometry.h/cpp as shapes
		[x] Rename file (and stuff within, if neeeded)
		[x] Rename 'import "geometry.h"' --> 'import "shapes.h"'
	[x] unittest_geometry.t.h & run_unittests_geometry.sh
		[x] Rename & endit
		[x] Run tests
	[x] compile program & run regression tests
	
	[x] Create geometry.h(cpp?) file & unit tests for Points, Vectors, Rays
		[x] Create unittest_geometry.t.h
		[x] Create run_unittest_geometry.sh
		
		[x] Define Point class
		[x] Write unit tests for Point class, and code to pass them
			[x] Write basic unit tests
			[x] Write more unit tests

		[x] Define Vector class
		[x] Write unit tests for Vector class, and code to pass them
			[x] Write basic unit tests
			[x] Write more unit tests
			
		[x] Write dot-product function for vectors

		[x] Define Ray class
		[x] Write unit tests for Ray class, and code to pass them
			[x] Write basic unit tests
			[x] Write more unit tests

	[x] Rework code to use Point/Vector
		Vec3f --> Point or Vector
		vec3f.normalize() --> Normalize(vec3f)
		substitute for updated dot-product function
		
		[x] ID files needing updating
			[x] materials.h
			[x] shapes.h
			[x] unittest_shapes.t.h

			[x] cameras.h
				position, direction, GenerateCameraRay
			[x] lights.h
			[x] unittest_lights.t.h
			[x] scene.h
			[x] shapes.cpp
			[x] render.cpp
			[x] scenefile_parser.cpp


	1. Replace Vec3f with Point or Vector, as appropriate
	2. LATER: replace camera origin, raydir, depth with Ray


Preliminary transformation matrices
	-- probably best to base this on PBRT approach
	
	1. Initial implementation using null/stub transforms (which return positions
	and vectors unmodified)
	2. Replace this with matrix-transformation, but still using null transform
	
	Intersection tests of camera rays with objects
		-- simplest to do this in *object local* space
		Convert from world coords (ray origin, direction) to object coords
	Test using spheres
		-- only positional offsets should matter
		-- easy comparison with existing renderings
	[ ] Add to Shape class: test for presence of Transform pointer; if not
	present, default to no-op
	
	[ ] NOTE: Current code generally assumes that objects are in *front* of
	camera (e.g, Sphere intersection tests for t_ca >= 0 to see if intersection
	is in front of camera). But when we use WorldToObject inside the Intersect
	methods, directions are *reversed* -- i.e., instead of camera ray origin
	at (0,0,0) and sphere's center at (0, -1, -30), the ray origin is at (0, -1, -30)
	and the sphere's center is at (0,0,0). This means that the returned t
	value for a hit is wrong, and so RayTrace()'s calculation of p_hit will
	be wrong...
	
	[ ] Also need to modify Sphere::GetNormalAtPoint (and for other subclasses)
	so it internally converts the input Point from world to object coords.


New Ray class
	Currently defined in geometry.h, but not yet used
	origin (point vector)
	direction vector
	most recent IOR passed through (for helping to determine refraction)
		Use: when generating a refraction ray, compute refraction using IOR
		of object/scene ray is going into and IOR of what the ray passed
		through just before hitting the refraction point.
		Non-ray/path-tracing Rays have currentIOR = 0 or somesuch;
		Camera rays have currentIOR = scene IOR.

[..]Implement lights.
		[X] Implement DistantLight
		Implement spotlight? (see PBRT)


[..] Images as textures
	[ ] Code to read and store an image
		[ ] Write interface
		[ ] Write simple unit test
			[ ] Write code for unit test
			[ ] Generate simple (4x4 RGB?) test image
			[ ] Write code to make unit test pass
	
	[ ] Image as background
	
	[ ] Image as texture for plane


[ ] Move color computation into Object (or its Material instance)
	-- needed for diffuse color:
		object/material intrinsic surfaceColor
		n_hit
		lightDirection
	-- needed for specular color:
		incoming ray direction (raydir)
		estimate of reflection ray direction (refldir)
			Vec3f refldir = raydir - n_hit*2*raydir.dotProduct(n_hit);

	So: input to material's ComputeColor function:
		raydir, n_hit, lightDirection
		-- and then we can compute everything in the Material object
	And then in RayTrace(), we multiply the result by (lightIntensity * visibility)
	to get the final (non-reflection,non-refraction) Color value
	
	Color MatteMaterial::ComputeObjectColor( Vec3f rayDirection, Vec3f n_hit, Vec3f lightDirection )
	{
      return surfaceColor * std::max(float(0), n_hit.dotProduct(-lightDirection));
	}		

	Color PhongMaterial::ComputeObjectColor( Vec3f rayDirection, Vec3f n_hit, Vec3f lightDirection )
	{
      Color diffuseColor = surfaceColor * std::max(float(0), n_hit.dotProduct(-lightDirection));
      Vec3f refldir = rayDirection - n_hit*2*rayDirection.dotProduct(n_hit);  // reflection direction
      refldir.normalize();
      Color specularColor = std::pow(std::max(0.f, refldir.dotProduct(-rayDirection)), phongExponent);
      
      return Kd*diffuseColor + Ks*specularColor;
	}		


[X]Implement plane with intersection.


Implement disk with intersection.


Implement box object with intersection


[ ] Implement visible lights (for area lights)
		SphereList and RectLight (and DiskLight, if we make that) should be visible
		Need to be part of object list as objects with emissivity
	

Implement camera re-orientation (see scratchapixel.com v2 notes: camera is at
(0,0,0) in its native frame; translation to (x_cam,y_cam,z_cam) in world frame + 
rotation.
	Simplest approach may be to follow scratchapixel: compute everything as
	currently done, then transform pixel position from camera space to world
	space, subtract from (transformed) camera position to get direction ray in
	world space.


Save alpha channel.


Save depth channel.


[-] Replace our Mersenne Twister with C++11 library implementation? (random::mt19937)
	-- NO: this is too arcane and complicated, for no
	-- besides, we might want to later try using pcg32, which is both faster,
	arguably better RNG, *and* easier to use.
	
? Replace Mersenne Twister with faster RNG?
	e.g., https://github.com/wjakob/pcg32/blob/master/pcg32.h


WILD FUTURE IDEAS:

Attenuation for transparent materials via Beer's Law (see notes in render.cpp)
	Need to check to see how attenuation coefficients map to observed color of
	tinted materials (water, tinted glass, etc.)
	Look in pbrt code?
	
Adaptive sub-sampling
	Compare intensities of samples for a given pixel; if dispersion is too large,
	generate extra samples for that pixel

	
Read in OBJ files? (e.g., with tinyobj library)

Read in (simple) RIB files??


(Uni-directional) path tracing??



ASSORTED NOTES:

World/scene orientation:

	For default camera orientation (e.g., for scene files, etc.):
		x-axis: left-right, increasing to right
		y-axis: up-down, increasing upwards [NOTE: this is *opposite* to pixel coords on image]
		z-axis: in-out, increasing *outward* (toward viewer); distance *into* scene,
			away from viewer, is *negative*
		
		This follows proper right-hand rule: +x vector x +y vector = +z vector
		

Direction vectors:
	+++ Vector pointing from A to B is B - A: +++
		B - A = (x_b - x_a, y_b - y_a, z_b - z_a)
		
		Ex: direction vector from origin to point P:
		dir(O->P) = P - O = (x_p - 0, y_p - 0, z_p - 0) = (x_p, y_p, z_p) [i.e., same components as P]


*** Image orientation:
	Internally, (x,y) = (0,0) becomes the *upper left* pixel in the output image
	(e.g., PNG displayed in Preview)
		==> small x = left side of image
		    small y = *upper* part of image



*** Camera rays:

	Image plane = (forward-projected) focal plane, located at z = 1 from camera
	origin (z = -1 in camera coordinate space)
	Camera origin = location of pinhole/lens
	
	The camera ray has its origin at the camera position (more precisely, at the
	camera *pinhole/lens*, and a direction defined by subtracting origin from current 
	pixel coordinate (output from Sampler), after the pixel coordinate has been 
	transformed from image ("raster") space to world space.
		raydir = pt_pixel_world - origin_cam_world
	
	Converting 2D sample coordinate in raster space to 3D position in camera space
	1. 2D Raster-space coordinates: x_pix, y_pix (generated by Sampler object)
								spans (0--npix_x - 1, 0--npix_y - 1)
	2. 2D Normalized device coordinates (NDC; (spans 0--1,0--1)): x_ndc, y_ndc 
			x_ndc = (x_pix + 0.5)/width
			y_ndc = (y_pix + 0.5)/height
	3. 2D Screen space (spans -1--1, -1--1): x_scr, y_scrn
			x_scrn = 2*x_ndc - 1
			y_scrn = -(2*y_ndc - 1)   [negative so we get y_scrn = -1 at *bottom* of image]
	4. 3D Camera space coordinates: x_cam, y_cam, z_cam
		(note that by construction, the camera is oriented to point out along the
		-z axis, with the screen 1 unit away, so all screen points are at z = -1)
			x_cam = x_scrn * tanTheta * aspectRatio
			y_cam = y_scrn * tanTheta
			z_cam = -1

	Thus,
		raydir = (x_cam, y_cam, z_cam) - (0,0,0) -- in camera coordinates


*** Area lights:

Idea is to do Monte Carlo integration to solve the problem: how much of the
area light's area can I actually see from a given point (taking into account
possible occlusion by other objects).

Ideally, we want to sample uniformly (and probably randomly) from the
projected surface of the area light. 

This is easy to do for a sphere, since its projected surface is always a circle.
	Sampling: ray to center of sphere + random offset, such that offset is
	always < R
	
	Easiest approach for raytracer case is probably to randomly generate *points on the
	surface of the sphere*, so that we can compute ray directions as p_surfsphere - p_0
	(where p_0 is the current position we're investigating)
	Strictly speaking we should generate points on the hemisphere facing the point,
	but because of the sphere's symmetry, it doesn't matter if we do the much
	easier thing and generate random points on the entire surface
	
	Random point on sphere = center + vector
		vector = random direction with length R
		
		OK, the correct way to do this is not obvious; 
		see here: http://mathworld.wolfram.com/SpherePointPicking.html
		1. Generate random variables theta between 0 and 2 pi and u [= cos phi] between -1 and 1
		2. x = sqrt(1 - u^2) * cos(theta)
		   y = sqrt(1 - u^2) * sin(theta)
		   z = u
		
		To get vector with length R, just do
			dir = R * Vec3f(x, y, z)
		   


*** Lights and light falloff/attenuation (acc. to PBRT)

The standard for light transmitted by rays is "radiance" (= astronomical *intensity*),
which remains constant along rays (through open space) regardless of distance.
This applies to all light reflected from (or refracted through) surfaces.

For distant lights, the light value (internal data member "L") *is*
radiance, and is unaffected by distance.
Same for diffuse lights (internal data member "Lemit").

For *point lights*, the light value (internal data member "L") is
equivalent to luminosity, and *is* affected by distance in the expected
1/d^2 fashion.

For spotlights, the light value (internal data member "Intensity") is also
equivalent to luminosity, with same 1/d^2 falloff as for point light. (I think
the idea is that a spotlight is essentially a point light with emission
restricted to a cone.)

PBRT's GonioPhotometricLight behaves same as point/spot light in this sense;
again, I think the idea is that it's a point light with a specific angular
emission function.

It appears that for *area* lights, the light value is *radiance* (think of
how surface brightness relates to flux: more distance galaxy has same
surface brightness, but occupies smaller angle, thus smaller received flux).



*** Translating photometric terms:

PBRT		other			astronomy
=====================================
xxx							---
xxx							luminosity
irradiance					flux
xxx							---
radiance					intensity



*** BUGS FIXED:

[X] Currently, when a light (Point, probably other) has too low a y value for its
position, there's a weird cutoff in the illumination
this seems to happen when there is a sphere *and* a plane; but without the
plane, the sphere is illuminated correctly!
	[] Check to see if occulted pixels are *blocked* (and if so, by what object),
	or if something else is going on 
	
	HERE'S what seems to be happening:
		Points sufficiently high on sphere that their lightDirection vectors
		point down (dy < 0) have TraceShadowRay intersections with the *plane*
		(object index j = 1) where, for some reason, 
			t_0,t_1 = 26.701248,-437964930662619152384.000000
		The t_1 value is WRONG; since it is negative and therefore
		less than the distance to the light, this triggers blocked=true
		
		So the problem may be with the intersect method of the Plane class...
		
		OK, it appears that Plane::intersect never sets a value for t_1 if there's
		an intersection!
		This is a problem, because *if* there's an intersection, the code in
		TraceShadowRay compares the values of *both* t_0 *and* t_1 to the distance
		to the light.
		
		SOLUTION: Always set t1 in intersect() if an intersection is found; if no
		second intersection is possible, then set t1 = t0

EXAMPLE:
./raytracer2 --width=150 --height=100 tests/scene_sphere-point-light-left.yml -o test_point-left_big.exr


upper-left part of sphere (bad):  x = 5, y = 4:  lightDirection = (0.919428,0.012680,0.393054):  blocked = 1
lower-left part of sphere (good):  x = 5, y = 6:  lightDirection = (0.858244,-0.433943,-0.274064):  blocked = 0

